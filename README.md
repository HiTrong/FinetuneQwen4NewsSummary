# Finetuning Qwen for News Summarization

---

## ðŸ§  Objective

This project focuses on fine-tuning a lightweight instruction-tuned language model, **Qwen2.5-0.5B-Instruct**, for the task of **Vietnamese news summarization**. The goal is to enable the model to generate concise and semantically accurate summaries from raw news content.

---

## ðŸ—ï¸ Model & Fine-tuning Setup

- **Base Model**: [`unsloth/Qwen2.5-0.5B-Instruct-bnb-4bit`](https://huggingface.co/unsloth/Qwen2.5-0.5B-Instruct-bnb-4bit)
- **Approach**: LoRA (Low-Rank Adaptation) with 4-bit quantization using `bitsandbytes`
- **Prompt Format**:
  - Instruction-style input (`Content â†’ Summary`)
  - Few-shot, task-specific formatting with system, user, and assistant messages
- **Why Qwen2.5-0.5B**:
  - Lightweight and efficient (0.5B parameters)
  - Long-context support (up to 8192 tokens)
  - Finetune-able on consumer-grade GPUs

---

## ðŸ§‘â€ðŸ’» Training Details

- **Environment**: Kaggle GPU (NVIDIA Tesla P100 16GB)
- **Libraries**: `Unsloth`, `transformers`, `bitsandbytes`, `peft`
- **Dataset**: [`nhantruongcse/summary-vietnamese-news`](https://huggingface.co/datasets/nhantruongcse/summary-vietnamese-news)
- **Subset Size**: 10,000 training samples, 2,000 test samples

### ðŸ§¾ Sample Prompt Template

```python
system_message = """Báº¡n lÃ  trá»£ lÃ½ AI cÃ³ nhiá»‡m vá»¥ tÃ³m táº¯t vÄƒn báº£n cá»§a ngÆ°á»i dÃ¹ng cung cáº¥p.

### HÆ°á»›ng dáº«n tráº£ lá»i:
KhÃ´ng dÃ i dÃ²ng, giá»›i háº¡n 100 tá»«
KhÃ´ng giáº£i thÃ­ch gÃ¬ thÃªm, chá»‰ cáº§n tÃ³m táº¯t
"""

user_template = """HÃ£y tÃ³m táº¯t vÄƒn báº£n dÆ°á»›i Ä‘Ã¢y:

{Content}
"""

assistant_template = """{Summary}
"""
```

---

## ðŸ“‰ Training Progress

| Epoch | Time |
|-------|-----------------------|
| 1     | 01:19:51               |

| Step | Training Loss |
|------|---------------|
|  30  | 2.493200      |
|  60  | 2.292400      |
|  90  | 2.273000      |
| 120  | 2.253500      |
| 150  | 2.237000      |
| 180  | 2.258400      |
| 210  | 2.231500      |
| 240  | 2.242900      |
| 270  | 2.233000      |
| 300  | 2.254700      |

- **Final Loss:** ~2.25
- **Estimated Perplexity:** exp(2.25) â‰ˆ 9.5

## ðŸ” Evaluate Pretrained and Finetuned Model Inference

### ðŸ–¼ï¸ Visual Comparison

#### ðŸ”· Boxplot: Distribution of Evaluation Metrics
![Boxplot Comparison](./evaluate/comparison_metrics.png)

#### ðŸ”¶ Bar Chart: Average Scores by Metric
![Barplot Comparison](./evaluate/comparison_metrics2.png)

These plots demonstrate that the finetuned model consistently outperforms the pretrained model across all metrics.


## ðŸ“‹ Statistical Test Results (Paired t-test)

| **Metric**     | **t-statistic** | **p-value** | **Significance**              |
|----------------|----------------:|------------:|-------------------------------|
| **ROUGE-1**     | 56.347          | 0.000       | âœ… Significant improvement     |
| **ROUGE-2**     | 12.732          | 0.000       | âœ… Significant improvement     |
| **ROUGE-L**     | 37.449          | 0.000       | âœ… Significant improvement     |
| **ROUGE-Lsum**  | 37.449          | 0.000       | âœ… Significant improvement     |
| **BERTScore**   | 30.142          | 0.000       | âœ… Significant improvement     |

> All metrics show statistically significant improvements after fine-tuning (**p < 0.001**).


## ðŸ“ˆ Analysis

The paired t-test confirms that fine-tuning yields **statistically significant performance gains** across all key summarization metrics:

- ðŸ”¹ **ROUGE-1 / ROUGE-2**: Improved n-gram overlap with reference summaries.
- ðŸ”¹ **ROUGE-L / ROUGE-Lsum**: Better sequence alignment and structural coherence.
- ðŸ”¹ **BERTScore**: Higher semantic similarity, showing the model better understands context and meaning.

These improvements demonstrate that the fine-tuned model produces **more accurate, fluent, and semantically faithful summaries** compared to the pretrained baseline.


## âœ… Conclusion

Fine-tuning the Qwen2.5-0.5B-Instruct model on Vietnamese news data has clearly enhanced its ability to generate high-quality summaries. The model shows significant improvements both statistically and visually, validating the effectiveness of the LoRA fine-tuning approach combined with 4-bit quantization.
