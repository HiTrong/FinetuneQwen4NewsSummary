# Finetune Qwen-2.5-0.5B-Instruct model

---

## üß† Objective & Model

- **Objective:** Fine-tune a language model to generate concise summaries from news content (`Content ‚Üí Summary`).
- **Base model:** `unsloth/Qwen2.5-0.5B-Instruct-bnb-4bit`
- **Why this model:** Lightweight, supports long context (up to 8192 tokens), optimized for consumer GPUs.
- **Fine-tuning approach:** LoRA (Low-Rank Adaptation) + 4-bit quantization using `bitsandbytes`.


## ü¶æ Training Environment

This model was finetuned on Kaggle using:

- **GPU:** NVIDIA Tesla P100 16GB
- **Frameworks/Libs**: Unsloth, HuggingFace,...
- **Techniques**: Quantization, LoRA,...


## üìö Dataset

- **Name:** [nhantruongcse/summary-vietnamese-news](https://huggingface.co/datasets/nhantruongcse/summary-vietnamese-news)
- **Format:** Contains two columns: `Content` (article body) and `Summary` (human-written summary)
- **Original size:** 164,573 rows
- **Subset:** 10,000 examples (randomly selected)
- **Split:**
  - Train: 10,000
  - Test: 2,000

- **Prompt Template:**
```python
system_message = """B·∫°n l√† tr·ª£ l√Ω AI c√≥ nhi·ªám v·ª• t√≥m t·∫Øt vƒÉn b·∫£n c·ªßa ng∆∞·ªùi d√πng cung c·∫•p.

### H∆∞·ªõng d·∫´n tr·∫£ l·ªùi:
Kh√¥ng d√†i d√≤ng, gi·ªõi h·∫°n 100 t·ª´
Kh√¥ng gi·∫£i th√≠ch g√¨ th√™m, ch·ªâ c·∫ßn t√≥m t·∫Øt
"""

user_template = """H√£y t√≥m t·∫Øt vƒÉn b·∫£n d∆∞·ªõi ƒë√¢y:

{Content}
"""

assistant_template = """{Summary}
"""
```


## üìâ Training Result

| Epoch | Time |
|-------|-----------------------|
| 1     | 01:19:51               |

| Step | Training Loss |
|------|---------------|
|  30  | 2.493200      |
|  60  | 2.292400      |
|  90  | 2.273000      |
| 120  | 2.253500      |
| 150  | 2.237000      |
| 180  | 2.258400      |
| 210  | 2.231500      |
| 240  | 2.242900      |
| 270  | 2.233000      |
| 300  | 2.254700      |

- **Final Loss:** ~2.25
- **Estimated Perplexity:** exp(2.25) ‚âà 9.5